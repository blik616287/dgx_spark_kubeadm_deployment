# Use KServe InferenceService (true) or plain Deployments (false)
kserve: true

namespace: llm-serving

# On DGX Spark (GB10 unified memory), GPU VRAM and system memory share
# the same 128GB pool. Pod memory limits must cover model weights,
# KV cache, and Ollama overhead since they all count against the cgroup.

models:
  qwen3-coder-next:
    enabled: true
    image: ollama/ollama:latest
    ollamaPull: "qwen3-coder-next:q4_K_M"
    displayName: qwen3-coder-next
    port: 11434
    numCtx: "131072"
    resources:
      requests:
        cpu: "4"
        memory: 1Gi
      limits:
        cpu: "10"
        memory: 120Gi
        nvidia.com/gpu: "1"
    numParallel: "1"
    persistence:
      storageClass: longhorn-models
      size: 60Gi

  deepseek-r1-distill-32b:
    enabled: true
    image: ollama/ollama:latest
    ollamaPull: "deepseek-r1:32b"
    displayName: deepseek-r1-distill-32b
    port: 11434
    numCtx: "32768"
    # Create a tools-enabled variant using a custom Modelfile
    toolsAlias: "deepseek-r1-distill-32b-tools"
    toolsModelfile: "deepseek-tools.Modelfile"
    resources:
      requests:
        cpu: "4"
        memory: 1Gi
      limits:
        cpu: "10"
        memory: 120Gi
        nvidia.com/gpu: "1"
    numParallel: "1"
    persistence:
      storageClass: longhorn-models
      size: 30Gi

nodeSelector:
  kubernetes.io/arch: arm64
