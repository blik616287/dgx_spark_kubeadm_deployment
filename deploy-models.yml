---
- name: Deploy LLMs via KServe on Kubernetes
  hosts: k8s
  become: true
  vars:
    local_user: "{{ ansible_env.SUDO_USER | default('b') }}"
    kubeconfig: "/home/{{ ansible_env.SUDO_USER | default('b') }}/.kube/config"
    models_dir: "/misc/models"
    chart_dir: "{{ playbook_dir }}/charts/llm-serving"
    # Set to true to use KServe InferenceService, false for plain Deployments
    use_kserve: true

  environment:
    KUBECONFIG: "{{ kubeconfig }}"

  tasks:
    # ── Verify models are downloaded ──────────────────────────────
    - name: Check Qwen3-Coder-Next weights exist
      find:
        paths: "{{ models_dir }}/qwen3-coder-next"
        patterns: "*Q4_K_M*.gguf"
      register: qwen_files

    - name: Check DeepSeek-R1-Distill weights exist
      find:
        paths: "{{ models_dir }}/deepseek-r1-distill-32b"
        patterns: "*Q4_K_M*.gguf"
      register: deepseek_files

    - name: Fail if models not downloaded
      fail:
        msg: >
          Model weights not found. Run download-models.yml first:
          ansible-playbook -i inventory.ini download-models.yml --become
      when: qwen_files.matched == 0 or deepseek_files.matched == 0

    - name: Show found model files
      debug:
        msg:
          - "Qwen3: {{ qwen_files.files | map(attribute='path') | list }}"
          - "DeepSeek: {{ deepseek_files.files | map(attribute='path') | list }}"

    # ── Install cert-manager (KServe prerequisite) ────────────────
    - name: Add Jetstack Helm repo
      become: true
      become_user: "{{ local_user }}"
      command: helm repo add jetstack https://charts.jetstack.io
      register: jetstack_repo
      changed_when: "'has been added' in jetstack_repo.stdout"
      failed_when: false

    - name: Update Helm repos
      become: true
      become_user: "{{ local_user }}"
      command: helm repo update

    - name: Check if cert-manager is installed
      become: true
      become_user: "{{ local_user }}"
      command: helm status cert-manager -n cert-manager
      register: certmgr_status
      failed_when: false
      changed_when: false

    - name: Install cert-manager
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install cert-manager jetstack/cert-manager \
          --namespace cert-manager \
          --create-namespace \
          --set crds.enabled=true \
          --wait --timeout 5m
      when: certmgr_status.rc != 0

    # ── Install KServe ────────────────────────────────────────────
    - name: Check if KServe CRDs are installed
      become: true
      become_user: "{{ local_user }}"
      command: kubectl get crd inferenceservices.serving.kserve.io
      register: kserve_crd_check
      failed_when: false
      changed_when: false

    - name: Install KServe CRDs
      become: true
      become_user: "{{ local_user }}"
      command: >
        helm upgrade --install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd
        --version v0.16.0
        --wait --timeout 5m
      when: kserve_crd_check.rc != 0

    - name: Check if KServe controller is installed
      become: true
      become_user: "{{ local_user }}"
      command: helm status kserve -n kserve
      register: kserve_status
      failed_when: false
      changed_when: false

    - name: Install KServe (RawDeployment mode)
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install kserve oci://ghcr.io/kserve/charts/kserve \
          --version v0.16.0 \
          --namespace kserve \
          --create-namespace \
          --set kserve.controller.deploymentMode=RawDeployment \
          --wait --timeout 5m
      when: kserve_status.rc != 0

    - name: Wait for KServe controller to be ready
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n kserve wait --for=condition=Ready pods --all --timeout=120s
      retries: 3
      delay: 10
      register: kserve_ready
      until: kserve_ready.rc == 0

    # ── Deploy LLM serving chart ──────────────────────────────────
    - name: Deploy llm-serving Helm chart
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install llm-serving {{ chart_dir }} \
          --namespace llm-serving \
          --create-namespace \
          --set modelsHostPath={{ models_dir }} \
          --wait --timeout 15m
      register: chart_install

    - name: Wait for model pods to be ready
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving get pods --field-selector=status.phase=Running -o name | \
        xargs -r kubectl -n llm-serving wait --for=condition=Ready --timeout=600s
      retries: 6
      delay: 30
      register: pods_ready
      until: pods_ready.rc == 0

    # ── Verify endpoints ──────────────────────────────────────────
    - name: Wait for Ollama to load models
      pause:
        seconds: 30

    - name: Verify Qwen3-Coder-Next is serving
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving exec deploy/qwen3-coder-next -- \
          curl -s http://localhost:11434/api/tags | python3 -c "
        import sys,json
        tags = json.load(sys.stdin)
        models = [m['name'] for m in tags.get('models',[])]
        print('Loaded models:', models)
        assert len(models) > 0, 'No models loaded'
        "
      register: qwen_verify
      retries: 12
      delay: 15
      until: qwen_verify.rc == 0

    - name: Verify DeepSeek-R1 is serving
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving exec deploy/deepseek-r1-distill-32b -- \
          curl -s http://localhost:11434/api/tags | python3 -c "
        import sys,json
        tags = json.load(sys.stdin)
        models = [m['name'] for m in tags.get('models',[])]
        print('Loaded models:', models)
        assert len(models) > 0, 'No models loaded'
        "
      register: deepseek_verify
      retries: 12
      delay: 15
      until: deepseek_verify.rc == 0

    - name: Show deployment status
      become: true
      become_user: "{{ local_user }}"
      shell: |
        echo "=== LLM Serving Pods ==="
        kubectl get pods -n llm-serving -o wide
        echo ""
        echo "=== Services ==="
        kubectl get svc -n llm-serving
        echo ""
        echo "=== InferenceServices ==="
        kubectl get inferenceservice -n llm-serving 2>/dev/null || echo "(KServe InferenceServices not deployed)"
        echo ""
        echo "=== Endpoints ==="
        echo "Qwen3-Coder-Next:      http://qwen3-coder-next.llm-serving.svc.cluster.local:11434"
        echo "DeepSeek-R1-Distill:   http://deepseek-r1-distill-32b.llm-serving.svc.cluster.local:11434"
        echo ""
        echo "=== Usage ==="
        echo 'curl http://qwen3-coder-next.llm-serving.svc.cluster.local:11434/v1/chat/completions \'
        echo '  -H "Content-Type: application/json" \'
        echo '  -d '"'"'{"model":"qwen3-coder-next","messages":[{"role":"user","content":"Write hello world in Python"}]}'"'"''
      register: status

    - name: Display status
      debug:
        var: status.stdout_lines
