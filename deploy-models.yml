---
- name: Deploy LLMs via KServe on Kubernetes
  hosts: k8s
  become: true
  vars:
    local_user: "{{ ansible_env.SUDO_USER | default('b') }}"
    kubeconfig: "/home/{{ ansible_env.SUDO_USER | default('b') }}/.kube/config"
    chart_dir: "{{ playbook_dir }}/charts/llm-serving"

  environment:
    KUBECONFIG: "{{ kubeconfig }}"

  tasks:
    # ── Verify model PVCs exist ──────────────────────────────────
    - name: Check model PVCs are bound
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving get pvc ollama-qwen3-coder-next ollama-deepseek-r1-distill-32b --no-headers 2>/dev/null | grep -c Bound
      register: pvc_check
      failed_when: false
      changed_when: false

    - name: Warn if model PVCs not populated
      debug:
        msg: >
          Model PVCs not found or not bound in llm-serving namespace.
          Run download-models.yml first for pre-populated models.
      when: pvc_check.stdout | default('0') | int < 2

    # ── Install cert-manager (KServe prerequisite) ────────────────
    - name: Add Jetstack Helm repo
      become: true
      become_user: "{{ local_user }}"
      command: helm repo add jetstack https://charts.jetstack.io
      register: jetstack_repo
      changed_when: "'has been added' in jetstack_repo.stdout"
      failed_when: false

    - name: Update Helm repos
      become: true
      become_user: "{{ local_user }}"
      command: helm repo update

    - name: Check if cert-manager is installed
      become: true
      become_user: "{{ local_user }}"
      command: helm status cert-manager -n cert-manager
      register: certmgr_status
      failed_when: false
      changed_when: false

    - name: Install cert-manager
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install cert-manager jetstack/cert-manager \
          --namespace cert-manager \
          --create-namespace \
          --set crds.enabled=true \
          --wait --timeout 5m
      when: certmgr_status.rc != 0

    # ── Install KServe ────────────────────────────────────────────
    - name: Check if KServe CRDs are installed
      become: true
      become_user: "{{ local_user }}"
      command: kubectl get crd inferenceservices.serving.kserve.io
      register: kserve_crd_check
      failed_when: false
      changed_when: false

    - name: Install KServe CRDs
      become: true
      become_user: "{{ local_user }}"
      command: >
        helm upgrade --install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd
        --version v0.16.0
        --wait --timeout 5m
      when: kserve_crd_check.rc != 0

    - name: Check if KServe controller is installed
      become: true
      become_user: "{{ local_user }}"
      command: helm status kserve -n kserve
      register: kserve_status
      failed_when: false
      changed_when: false

    - name: Install KServe (RawDeployment mode)
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install kserve oci://ghcr.io/kserve/charts/kserve \
          --version v0.16.0 \
          --namespace kserve \
          --create-namespace \
          --set kserve.controller.deploymentMode=RawDeployment \
          --wait --timeout 5m
      when: kserve_status.rc != 0

    - name: Wait for KServe controller to be ready
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n kserve wait --for=condition=Ready pods --all --timeout=120s
      retries: 3
      delay: 10
      register: kserve_ready
      until: kserve_ready.rc == 0

    # ── Deploy LLM serving chart ──────────────────────────────────
    - name: Deploy llm-serving Helm chart
      become: true
      become_user: "{{ local_user }}"
      shell: |
        helm upgrade --install llm-serving {{ chart_dir }} \
          --namespace llm-serving \
          --create-namespace \
          --wait --timeout 15m
      register: chart_install

    - name: Wait for model pods to be ready
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving get pods --field-selector=status.phase=Running -o name | \
        xargs -r kubectl -n llm-serving wait --for=condition=Ready --timeout=600s
      retries: 6
      delay: 30
      register: pods_ready
      until: pods_ready.rc == 0

    # ── Verify models are registered ─────────────────────────────
    - name: Verify Qwen3-Coder-Next model is registered
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving exec deploy/qwen3-coder-next-predictor -- ollama list
      register: qwen_verify
      retries: 12
      delay: 15
      until: qwen_verify.rc == 0 and 'qwen3-coder-next' in qwen_verify.stdout

    - name: Verify DeepSeek-R1 model is registered
      become: true
      become_user: "{{ local_user }}"
      shell: |
        kubectl -n llm-serving exec deploy/deepseek-r1-distill-32b-predictor -- ollama list
      register: deepseek_verify
      retries: 12
      delay: 15
      until: deepseek_verify.rc == 0 and 'deepseek-r1-distill-32b-tools' in deepseek_verify.stdout

    - name: Show deployment status
      become: true
      become_user: "{{ local_user }}"
      shell: |
        echo "=== LLM Serving Pods ==="
        kubectl get pods -n llm-serving -o wide
        echo ""
        echo "=== InferenceServices ==="
        kubectl get inferenceservice -n llm-serving
        echo ""
        echo "=== Loaded Models ==="
        echo "Qwen3:"
        kubectl -n llm-serving exec deploy/qwen3-coder-next-predictor -- ollama list 2>/dev/null || echo "  (not ready)"
        echo "DeepSeek:"
        kubectl -n llm-serving exec deploy/deepseek-r1-distill-32b-predictor -- ollama list 2>/dev/null || echo "  (not ready)"
        echo ""
        echo "=== Cluster Endpoints ==="
        echo "Qwen3-Coder-Next:    http://qwen3-coder-next-predictor.llm-serving.svc.cluster.local:11434"
        echo "DeepSeek-R1-Distill: http://deepseek-r1-distill-32b-predictor.llm-serving.svc.cluster.local:11434"
      register: status

    - name: Display status
      debug:
        var: status.stdout_lines
