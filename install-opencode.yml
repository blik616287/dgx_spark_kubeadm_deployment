---
- name: Install OpenCode and configure for local LLM inference
  hosts: k8s
  become: true
  vars:
    local_user: "{{ ansible_env.SUDO_USER | default('b') }}"
    kubeconfig: "/home/{{ ansible_env.SUDO_USER | default('b') }}/.kube/config"
    opencode_config_dir: "/home/{{ ansible_env.SUDO_USER | default('b') }}/.config/opencode"

  environment:
    KUBECONFIG: "{{ kubeconfig }}"

  tasks:
    # ── Install OpenCode ─────────────────────────────────────────
    - name: Check if opencode is installed
      become: true
      become_user: "{{ local_user }}"
      shell: "PATH=$HOME/.opencode/bin:$PATH which opencode"
      register: opencode_check
      failed_when: false
      changed_when: false

    - name: Install opencode from opencode.ai
      become: true
      become_user: "{{ local_user }}"
      shell: curl -fsSL https://opencode.ai/install | bash
      when: opencode_check.rc != 0

    - name: Add opencode to PATH in shell profile
      become: true
      become_user: "{{ local_user }}"
      lineinfile:
        path: "/home/{{ local_user }}/.profile"
        line: 'export PATH="$HOME/.opencode/bin:$PATH"'
        create: true

    - name: Verify opencode installed
      become: true
      become_user: "{{ local_user }}"
      shell: "$HOME/.opencode/bin/opencode -v"
      register: opencode_version
      changed_when: false

    - name: Show opencode version
      debug:
        msg: "{{ opencode_version.stdout }}"

    # ── Expose models via NodePort ───────────────────────────────
    - name: Create NodePort service for Qwen3-Coder-Next
      become: true
      become_user: "{{ local_user }}"
      shell: |
        cat <<'EOF' | kubectl apply -f -
        apiVersion: v1
        kind: Service
        metadata:
          name: {{ models.coding.name }}-nodeport
          namespace: llm-serving
        spec:
          type: NodePort
          selector:
            app: isvc.{{ models.coding.name }}-predictor
          ports:
            - port: 11434
              targetPort: 11434
              nodePort: {{ models.coding.nodeport }}
        EOF

    - name: Create NodePort service for DeepSeek-R1
      become: true
      become_user: "{{ local_user }}"
      shell: |
        cat <<'EOF' | kubectl apply -f -
        apiVersion: v1
        kind: Service
        metadata:
          name: {{ models.reasoning.name }}-nodeport
          namespace: llm-serving
        spec:
          type: NodePort
          selector:
            app: isvc.{{ models.reasoning.name }}-predictor
          ports:
            - port: 11434
              targetPort: 11434
              nodePort: {{ models.reasoning.nodeport }}
        EOF

    - name: Verify NodePort services
      become: true
      become_user: "{{ local_user }}"
      shell: kubectl get svc -n llm-serving {{ models.coding.name }}-nodeport {{ models.reasoning.name }}-nodeport
      register: nodeport_svcs
      changed_when: false

    - name: Show NodePort services
      debug:
        var: nodeport_svcs.stdout_lines

    # ── Configure OpenCode ───────────────────────────────────────
    - name: Create opencode config directory
      file:
        path: "{{ opencode_config_dir }}"
        state: directory
        owner: "{{ local_user }}"
        mode: '0755'

    - name: Write opencode config
      copy:
        dest: "{{ opencode_config_dir }}/opencode.json"
        owner: "{{ local_user }}"
        mode: '0644'
        content: |
          {
            "$schema": "https://opencode.ai/config.json",
            "model": "qwen/{{ models.coding.tag }}",
            "small_model": "deepseek/{{ models.reasoning.tools_alias }}",
            "provider": {
              "qwen": {
                "npm": "@ai-sdk/openai-compatible",
                "name": "Qwen3 Coder Next (local)",
                "options": {
                  "baseURL": "http://localhost:{{ models.coding.nodeport }}/v1"
                },
                "models": {
                  "{{ models.coding.tag }}": {
                    "name": "{{ models.coding.name }}",
                    "limit": {
                      "context": 131072,
                      "output": 32768
                    }
                  }
                }
              },
              "deepseek": {
                "npm": "@ai-sdk/openai-compatible",
                "name": "DeepSeek R1 Distill 32B (local)",
                "options": {
                  "baseURL": "http://localhost:{{ models.reasoning.nodeport }}/v1"
                },
                "models": {
                  "{{ models.reasoning.tools_alias }}": {
                    "name": "{{ models.reasoning.name }}",
                    "limit": {
                      "context": 32768,
                      "output": 8192
                    }
                  }
                }
              }
            }
          }

    - name: Show setup summary
      debug:
        msg:
          - "OpenCode installed and configured"
          - "Default model: {{ models.coding.name }} at localhost:{{ models.coding.nodeport }}"
          - "Small model: {{ models.reasoning.name }} at localhost:{{ models.reasoning.nodeport }}"
          - "Config: {{ opencode_config_dir }}/opencode.json"
          - "Run 'opencode' to start"
